"""
Self-Critique Agent for RAG System Phase 2.

Validates generated answers before returning them to users by:
- Checking if the answer addresses the question
- Verifying citations and source grounding
- Detecting potential hallucinations
- Assessing overall answer quality
- Providing improvement suggestions

This agent acts as a quality gate to ensure RAG outputs meet standards.
"""
import logging
import re
from typing import Dict, Any, List, Optional
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

from src.generation.llm import LLMGenerator

logger = logging.getLogger(__name__)


# Self-Critique Prompt Template
CRITIQUE_PROMPT = """You are an expert evaluator assessing the quality of answers generated by a RAG system.

Your task is to critically evaluate an answer based on the question and source documents provided.

Question: {question}

Answer to Evaluate:
{answer}

Source Context:
{context}

Please evaluate the answer systematically on the following criteria:

1. ADDRESSES QUESTION: Does the answer directly address what was asked?
   - Consider if the answer is relevant and on-topic
   - Response: Yes or No

2. HAS CITATIONS: Does the answer reference or cite the source documents?
   - Look for implicit or explicit references to source material
   - Response: Yes or No

3. SUPPORTED: Is the answer supported by the provided context?
   - Check if claims are backed by the source documents
   - Identify any unsupported assertions
   - Response: Yes or No

4. HALLUCINATION RISK: What is the risk of hallucination in this answer?
   - Low: All claims directly from sources
   - Medium: Some inference but reasonable
   - High: Claims not supported by sources
   - Response: Low, Medium, or High

5. IMPROVEMENTS: What specific improvements would make this answer better?
   - List 2-3 concrete suggestions
   - Be specific and actionable

6. OVERALL QUALITY: What is the overall quality of this answer?
   - Poor: Doesn't address question, unsupported claims
   - Fair: Addresses question but has issues
   - Good: Solid answer with minor improvements possible
   - Excellent: Comprehensive, well-supported, accurate
   - Response: Poor, Fair, Good, or Excellent

Your response MUST follow this exact format:

Addresses Question: [Yes/No]
Has Citations: [Yes/No]
Supported: [Yes/No]
Hallucination Risk: [Low/Medium/High]
Improvements: [List 2-3 specific suggestions]
Overall Quality: [Poor/Fair/Good/Excellent]

Now, evaluate the answer:"""


class SelfCritiqueAgent:
    """
    Agent that critiques generated answers for quality and accuracy.

    Uses LLM-based evaluation to assess:
    - Question relevance
    - Citation presence
    - Source grounding
    - Hallucination risk
    - Overall quality

    Provides actionable improvement suggestions.
    """

    def __init__(
        self,
        llm: Optional[LLMGenerator] = None,
        temperature: float = 0.0,
        max_tokens: int = 500,
        max_context_length: int = 3000,
    ):
        """
        Initialize Self-Critique Agent.

        Args:
            llm: Optional LLM generator (creates one if not provided)
            temperature: LLM temperature (0.0 for deterministic evaluation)
            max_tokens: Maximum tokens for critique response
            max_context_length: Maximum context length to include in prompt
        """
        self.llm = llm or LLMGenerator(
            temperature=temperature,
            max_tokens=max_tokens,
            streaming=False,
        )
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.max_context_length = max_context_length

        logger.info(
            f"SelfCritiqueAgent initialized "
            f"(temperature={temperature}, max_tokens={max_tokens})"
        )

    def critique(
        self,
        question: str,
        answer: str,
        context: List[Document],
    ) -> Dict[str, Any]:
        """
        Critique a generated answer for quality and accuracy.

        Args:
            question: Original user question
            answer: Generated answer to evaluate
            context: Source documents used to generate answer

        Returns:
            Dictionary with critique results:
            - addresses_question: "Yes" or "No"
            - has_citations: "Yes" or "No"
            - supported: "Yes" or "No"
            - hallucination_risk: "Low", "Medium", or "High"
            - improvements: List of improvement suggestions
            - overall_quality: "Poor", "Fair", "Good", or "Excellent"

        Example:
            >>> agent = SelfCritiqueAgent()
            >>> critique = agent.critique(
            ...     question="What is RAG?",
            ...     answer="RAG combines retrieval with generation...",
            ...     context=[doc1, doc2]
            ... )
            >>> print(critique['overall_quality'])
            'Good'
        """
        if not answer or not answer.strip():
            logger.warning("Empty answer provided for critique")
            return {
                'addresses_question': 'No',
                'has_citations': 'No',
                'supported': 'No',
                'hallucination_risk': 'High',
                'improvements': ['Generate a non-empty answer'],
                'overall_quality': 'Poor',
            }

        # Build context string from documents
        context_str = self._format_context(context)

        # Format prompt
        prompt = CRITIQUE_PROMPT.format(
            question=question,
            answer=answer,
            context=context_str,
        )

        try:
            # Get LLM critique
            messages = [HumanMessage(content=prompt)]
            response = self.llm.generate(messages)

            # Parse structured response
            critique_result = self._parse_critique(response)

            logger.info(
                f"Critique complete: quality={critique_result['overall_quality']}, "
                f"hallucination_risk={critique_result['hallucination_risk']}"
            )

            return critique_result

        except Exception as e:
            logger.error(f"Error during critique: {str(e)}", exc_info=True)

            # Return fallback critique
            return {
                'addresses_question': 'Unknown',
                'has_citations': 'Unknown',
                'supported': 'Unknown',
                'hallucination_risk': 'Unknown',
                'improvements': [f'Critique failed: {str(e)}'],
                'overall_quality': 'Unknown',
                'error': str(e),
            }

    def _format_context(self, context: List[Document]) -> str:
        """
        Format context documents into a string for the critique prompt.

        Args:
            context: List of source documents

        Returns:
            Formatted context string
        """
        if not context:
            return "No context provided."

        # Build context string with length limiting
        context_parts = []
        total_length = 0

        for i, doc in enumerate(context, 1):
            doc_text = f"Document {i}:\n{doc.page_content}\n"

            # Check if adding this would exceed limit
            if total_length + len(doc_text) > self.max_context_length:
                remaining = self.max_context_length - total_length
                if remaining > 100:  # Only add if meaningful space left
                    doc_text = f"Document {i}:\n{doc.page_content[:remaining-50]}...\n"
                    context_parts.append(doc_text)
                break

            context_parts.append(doc_text)
            total_length += len(doc_text)

        return "\n".join(context_parts)

    def _parse_critique(self, response: str) -> Dict[str, Any]:
        """
        Parse structured critique response from LLM.

        Extracts the following fields using regex:
        - Addresses Question: Yes/No
        - Has Citations: Yes/No
        - Supported: Yes/No
        - Hallucination Risk: Low/Medium/High
        - Improvements: List of suggestions
        - Overall Quality: Poor/Fair/Good/Excellent

        Args:
            response: LLM's critique response text

        Returns:
            Dictionary with parsed critique fields

        Raises:
            ValueError: If required fields cannot be parsed
        """
        # Extract Addresses Question
        addresses_match = re.search(
            r'Addresses Question:\s*(Yes|No)',
            response,
            re.IGNORECASE
        )
        addresses_question = addresses_match.group(1) if addresses_match else "Unknown"

        # Extract Has Citations
        citations_match = re.search(
            r'Has Citations:\s*(Yes|No)',
            response,
            re.IGNORECASE
        )
        has_citations = citations_match.group(1) if citations_match else "Unknown"

        # Extract Supported
        supported_match = re.search(
            r'Supported:\s*(Yes|No)',
            response,
            re.IGNORECASE
        )
        supported = supported_match.group(1) if supported_match else "Unknown"

        # Extract Hallucination Risk
        hallucination_match = re.search(
            r'Hallucination Risk:\s*(Low|Medium|High)',
            response,
            re.IGNORECASE
        )
        hallucination_risk = hallucination_match.group(1) if hallucination_match else "Unknown"

        # Extract Improvements (may be multi-line)
        improvements_match = re.search(
            r'Improvements:\s*(.+?)(?=\nOverall Quality:|\n\n|$)',
            response,
            re.IGNORECASE | re.DOTALL
        )

        if improvements_match:
            improvements_text = improvements_match.group(1).strip()
            # Split by common list markers
            improvements = [
                imp.strip()
                for imp in re.split(r'\n[-â€¢*\d.]+\s*', improvements_text)
                if imp.strip()
            ]
            # If no list markers found, treat as single improvement
            if not improvements or improvements == [improvements_text]:
                improvements = [improvements_text]
        else:
            improvements = ["No specific improvements suggested"]

        # Extract Overall Quality
        quality_match = re.search(
            r'Overall Quality:\s*(Poor|Fair|Good|Excellent)',
            response,
            re.IGNORECASE
        )
        overall_quality = quality_match.group(1) if quality_match else "Unknown"

        # Validate that we got at least some fields
        if all(v == "Unknown" for v in [addresses_question, supported, overall_quality]):
            logger.warning(f"Could not parse critique response: {response[:200]}")
            raise ValueError(f"Could not parse required fields from response: {response[:200]}")

        return {
            'addresses_question': addresses_question,
            'has_citations': has_citations,
            'supported': supported,
            'hallucination_risk': hallucination_risk,
            'improvements': improvements,
            'overall_quality': overall_quality,
        }

    def should_refine(self, critique: Dict[str, Any]) -> bool:
        """
        Determine if an answer should be refined based on critique.

        Returns True if the answer has quality issues that warrant refinement:
        - Overall quality is Poor or Fair
        - Hallucination risk is High
        - Answer doesn't address the question
        - Answer is not supported by context

        Args:
            critique: Critique dictionary from critique() method

        Returns:
            True if answer should be refined, False otherwise

        Example:
            >>> critique = agent.critique(question, answer, context)
            >>> if agent.should_refine(critique):
            ...     # Generate improved answer
            ...     pass
        """
        # Check overall quality
        quality = critique.get('overall_quality', 'Unknown')
        if quality in ['Poor', 'Fair']:
            logger.info(f"Refinement recommended: quality is {quality}")
            return True

        # Check hallucination risk
        hallucination = critique.get('hallucination_risk', 'Unknown')
        if hallucination == 'High':
            logger.info("Refinement recommended: high hallucination risk")
            return True

        # Check if question is addressed
        addresses = critique.get('addresses_question', 'Unknown')
        if addresses == 'No':
            logger.info("Refinement recommended: doesn't address question")
            return True

        # Check if answer is supported
        supported = critique.get('supported', 'Unknown')
        if supported == 'No':
            logger.info("Refinement recommended: not supported by context")
            return True

        # Answer is acceptable
        logger.info(f"Answer quality acceptable: {quality}")
        return False

    def get_critique_summary(self, critique: Dict[str, Any]) -> str:
        """
        Get human-readable summary of critique results.

        Args:
            critique: Critique dictionary from critique() method

        Returns:
            Formatted summary string
        """
        summary_parts = [
            f"Overall Quality: {critique.get('overall_quality', 'Unknown')}",
            f"Addresses Question: {critique.get('addresses_question', 'Unknown')}",
            f"Supported by Context: {critique.get('supported', 'Unknown')}",
            f"Has Citations: {critique.get('has_citations', 'Unknown')}",
            f"Hallucination Risk: {critique.get('hallucination_risk', 'Unknown')}",
        ]

        improvements = critique.get('improvements', [])
        if improvements and improvements != ['No specific improvements suggested']:
            summary_parts.append("\nSuggested Improvements:")
            for i, imp in enumerate(improvements, 1):
                summary_parts.append(f"  {i}. {imp}")

        return "\n".join(summary_parts)

    def get_agent_config(self) -> Dict[str, Any]:
        """
        Get current agent configuration.

        Returns:
            Dictionary with agent parameters
        """
        return {
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'max_context_length': self.max_context_length,
            'llm_config': self.llm.get_model_info(),
        }


def create_self_critique_agent(
    temperature: float = 0.0,
    max_tokens: int = 500,
) -> SelfCritiqueAgent:
    """
    Factory function to create a Self-Critique Agent with default settings.

    Args:
        temperature: LLM temperature (0.0 for deterministic)
        max_tokens: Maximum tokens for critique

    Returns:
        Configured SelfCritiqueAgent instance

    Example:
        >>> agent = create_self_critique_agent()
        >>> critique = agent.critique(question, answer, context)
    """
    return SelfCritiqueAgent(
        temperature=temperature,
        max_tokens=max_tokens,
    )
