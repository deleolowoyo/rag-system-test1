# RAG System - Phase 1: Foundation

A production-ready Retrieval Augmented Generation (RAG) system built with LangChain, ChromaDB, and Claude.

## ğŸ¯ Overview

This is Phase 1 of a comprehensive RAG system that will eventually include:
- **Phase 1**: RAG Foundation (âœ… Current)
- **Phase 2**: Enhanced RAG with Agent Reasoning
- **Phase 3**: MCP Integration
- **Phase 4**: LangGraph Orchestration
- **Phase 5**: Production Hardening

## ğŸ—ï¸ Architecture

```
Documents â†’ Loader â†’ Splitter â†’ Embedder â†’ Vector Store
                                                â†“
User Query â†’ Embedder â†’ Retriever â†’ Context â†’ LLM â†’ Answer
```

### Components

1. **Ingestion Pipeline**
   - `loaders.py`: Multi-format document loading (PDF, DOCX, TXT, MD)
   - `splitters.py`: Intelligent recursive text splitting
   - `embedder.py`: OpenAI embedding generation

2. **Storage Layer**
   - `vector_store.py`: ChromaDB vector storage with persistence

3. **Retrieval Layer**
   - `retriever.py`: Similarity search with score thresholding

4. **Generation Layer**
   - `prompts.py`: RAG-optimized prompt templates
   - `llm.py`: Claude Sonnet 4 integration with streaming

5. **Pipeline Orchestration**
   - `pipeline.py`: End-to-end RAG workflow

## ğŸ“‹ Prerequisites

- Python 3.10+
- OpenAI API key (for embeddings)
- Anthropic API key (for Claude)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or navigate to project
cd rag-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy environment template
cp .env.template .env

# Edit .env and add your API keys
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
```

### 3. Run Example

```bash
python example_usage.py
```

This will:
1. Initialize the RAG pipeline
2. Ingest sample documents
3. Run example queries
4. Show streaming responses

## ğŸ’» Usage

### Basic Usage

```python
from src.pipeline import RAGPipeline

# Initialize pipeline
pipeline = RAGPipeline(collection_name="my_docs")

# Ingest documents
pipeline.ingest_documents(directory_path="./data/raw")

# Query
result = pipeline.query("What is the main topic?")
print(result['answer'])
```

### Advanced Usage

```python
# Query with custom retrieval settings
result = pipeline.query(
    question="Explain the methodology",
    top_k=5,  # Retrieve more documents
    return_sources=True,  # Include source citations
    return_context=True,  # Include retrieved context
)

# Access detailed results
print(f"Answer: {result['answer']}")
print(f"Sources: {result['num_sources']}")
for source in result['sources']:
    print(f"  - {source['metadata']['file_name']} (score: {source['score']:.3f})")
```

### Streaming Responses

```python
# Stream tokens as they're generated
for token in pipeline.query_stream("Summarize the key findings"):
    print(token, end='', flush=True)
```

## âš™ï¸ Configuration

Edit `src/config/settings.py` or use environment variables:

### Embedding Settings
- `EMBEDDING_MODEL`: OpenAI model (default: `text-embedding-3-small`)
- `EMBEDDING_DIMENSIONS`: Vector dimensions (default: `1536`)

### Text Splitting
- `CHUNK_SIZE`: Maximum tokens per chunk (default: `1000`)
- `CHUNK_OVERLAP`: Overlap between chunks (default: `200`)

### Retrieval
- `RETRIEVAL_TOP_K`: Number of documents to retrieve (default: `4`)
- `RETRIEVAL_SCORE_THRESHOLD`: Minimum similarity score (default: `0.7`)
- `SEARCH_TYPE`: Search algorithm (`similarity` or `mmr`)

### LLM
- `LLM_MODEL`: Claude model (default: `claude-sonnet-4-20250514`)
- `LLM_TEMPERATURE`: Generation temperature (default: `0.0`)
- `LLM_MAX_TOKENS`: Maximum response tokens (default: `2048`)

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_ingestion.py

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“Š Performance

### Typical Metrics (Phase 1)

- **Query Latency**: ~2-3 seconds end-to-end
- **Embedding Cost**: ~$0.02 per 1M tokens
- **Storage**: Local (ChromaDB), ~0 cost
- **LLM Cost**: ~$3 per 1M input tokens

### Optimization Tips

1. **Chunk Size**: Tune based on document type
   - Technical docs: 500-800 tokens
   - Narrative text: 1000-1500 tokens

2. **Top-K**: Balance precision vs recall
   - Precise answers: k=3-4
   - Comprehensive coverage: k=5-8

3. **Score Threshold**: Filter noise
   - High precision: 0.8+
   - Balanced: 0.7
   - High recall: 0.6

## ğŸ” Common Issues

### "No relevant documents found"
- **Cause**: Query embedding doesn't match document embeddings
- **Fix**: Use more specific queries or lower score threshold

### "Context too long"
- **Cause**: Retrieved chunks exceed LLM context window
- **Fix**: Reduce `top_k` or `chunk_size`

### "API Key Error"
- **Cause**: Missing or invalid API keys
- **Fix**: Check `.env` file has correct keys

## ğŸ“ Project Structure

```
rag-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py          # Configuration management
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ loaders.py           # Document loading
â”‚   â”‚   â”œâ”€â”€ splitters.py         # Text chunking
â”‚   â”‚   â””â”€â”€ embedder.py          # Embedding generation
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ vector_store.py      # Vector database
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ retriever.py         # Document retrieval
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ prompts.py           # Prompt templates
â”‚   â”‚   â””â”€â”€ llm.py               # LLM interface
â”‚   â””â”€â”€ pipeline.py              # Main orchestration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ingestion.py        # Unit tests
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Input documents
â”‚   â””â”€â”€ chromadb/                # Vector store persistence
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.template                # Environment template
â””â”€â”€ example_usage.py             # Usage examples
```

## ğŸ“ Key Learnings for Engineering Teams

### 1. **Text Splitting Strategy**
- Use recursive splitting for semantic coherence
- Always include overlap to preserve context
- Tune chunk size based on domain

### 2. **Embedding Consistency**
- **Critical**: Use same embedding model for documents AND queries
- Mismatched embeddings = poor retrieval

### 3. **Prompt Engineering**
- Ground LLM in retrieved context
- Require citations for verifiability
- Provide "I don't know" escape hatch

### 4. **Metadata Management**
- Preserve source information throughout pipeline
- Enable filtering by metadata (source, date, etc.)

### 5. **Error Handling**
- Validate inputs at each stage
- Graceful degradation (e.g., "no documents found")
- Comprehensive logging

## ğŸš§ Next Steps (Phase 2)

Phase 2 will add:
- **Query Rewriting**: LLM-enhanced query optimization
- **Multi-Query Retrieval**: Generate query variations
- **Re-ranking**: Improve retrieved document ordering
- **Agent Reasoning**: ReAct-style agent loops

## ğŸ“š Resources

- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [Anthropic Claude API](https://docs.anthropic.com/claude/reference/getting-started-with-the-api)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [RAG Best Practices](https://www.anthropic.com/index/claude-2-1-prompting#retrieval-augmented-generation-rag)

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

This is a teaching project. Focus areas for improvement:
- Additional document loaders (HTML, CSV, etc.)
- Alternative embedding models
- Evaluation metrics and benchmarks
- More comprehensive tests

---

**Status**: Phase 1 Complete âœ…  
**Next Phase**: Enhanced RAG with Agent Reasoning
